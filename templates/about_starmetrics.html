<div class="about">
	<div class="intro">
	  <h1>
	    About Star Metrics</h1>
	  <p><a href="about.php">Overview</a>&nbsp;&nbsp;|&nbsp;&nbsp;Star Metrics&nbsp;&nbsp;|&nbsp;&nbsp;<a href="about_topics.php">Topic Modeling</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a href="about_patents.php">Patents Database</a></p>
	</div><!-- /intro -->
	<p>
	</p>
	<ol>
	  <li><a href="#overview">Brief Overview of the STAR METRICS Project</a></li>
	  <li><a href="#level1">Brief Overview of STAR METRICS Level I</a></li>
	  <li><a href="#level2">STAR METRICS Level II</a></li>
	  <li><a href="#pilotprojects">STAR METRICS Pilot Projects</a></li>
	  </ol>
	<p>&nbsp;</p>
	<h2 style="text-align: center">
	  <a name="overview"></a>1. Brief Overview of the STAR METRICS Project</h2>
    <h3>What is STAR METRICS?</h3>
    <p>The STAR METRICS program<sup>1</sup> is a voluntary collaboration between Federal science agencies and research institutions to document how Federal science investments support knowledge creation, economic growth, workforce development and a broad range of societal outcomes.  The program does this by building a data infrastructure that brings together inputs, outputs, and outcomes from a variety of existing digital sources in an open empirical framework.  The goal is to enable participants to use consistent data, models and tools to generate analysis and reports for their respective stakeholders while minimizing agency costs and respondent burden. It is intended to complement and enhance anecdotal and qualitative evidence. </p>
    <p>
      STAR METRICS has two levels.  Level I documents the initial effect of S&amp;T investments on employment using administrative records from research institutions.  Level II builds on Level I by connecting sources of funding, recipients of funding, interactions among scientists–both in the public and private sector–and the products of research over time. </p>
    <p>Since its inception in June 2010, the program has grown both in size and scope. It is also generating further interest among Federal science agencies, OMB, Congress, and other national and international stakeholders </p>
    <h3>Why develop STAR METRICS?</h3>
    <p>
      The STAR METRICS program was created in direct response to the reporting requirements associated with the 2009 American Reinvestment and Recovery Act (ARRA) in addition to other Administration and Congressional requests. </p>
    <p>
      The passage of ARRA, and the focus on reporting the jobs associated with science investments, led to a number of concerns.  The first was the accuracy of the approach used in the reporting process.  The second was the limited nature of that reporting requirement, since science investments have been documented to have longer term impact in many areas, including scientific outcomes, such as the generation and adoption of new science, often measured by citations and patents, on economic outcomes, such as job creation, on the outcomes of the scientific workforce, as well as on social outcomes such as health and the environment.</p>
    <p>Science agencies also needed to respond to two Open Government memoranda: the first, a Presidential Memorandum on Transparency and Open Government (Jan 21, 2009) and the second the Office of Management and Budget (OMB) Memorandum Open Government Directive (Dec. 8, 2009). Two key provisions of the OMB Memorandum should be highlighted. Provision 1b states that agencies should publish information online in an open format that can be retrieved, downloaded, and searched. Provision 1c states that subject to valid restrictions, agencies should proactively use modern technology to disseminate useful information.  In addition, the White House Office of Management and Budget (OMB) and Office of Science and Technology Policy (OSTP) have requested that federal agencies develop outcome-oriented goals for their science and technology activities<sup>2</sup>. </p>
    <p>The challenge in responding to these requests is that R&amp;D data are currently drawn from disparate sources, using widely differing methodologies and approaches. Inputs, outputs and outcomes are not combined in a systematic fashion.  The development of consistent and reliable answers requires the use of common data sources and standardized methodologies for data cleaning and analysis.  At the same time, it is clear that continuing to require research institutions and principal investigators to manually report the outcomes of research was neither practicable nor desirable .  A recent study titled &quot;Reforming Regulation of Research Universities&quot; provides a good summary of the challenges; it finds that poorly-integrated federal reporting and other regulations are imposing a heavy and growing administrative burden on federally-funded research. The report argues that this &quot;regulatory overhead&quot; is both large (and getting larger), and often inefficient, with many federal reporting requirements overlapping and even conflicting.  It estimates that 42% of faculty time relating to federally-funded research is spent on administrative duties, rather than on the research itself.</p>
    <p>Although most scientists&rsquo; activities could be captured electronically, the current reporting of research outputs is manual in nature.  Manual reporting is not only burdensome and costly, but also affects data quality through under-reporting and misreporting. It is time to use new digital technologies to capture the broad scientific, social, economic, and workforce results of Federal S&amp;T investments Research institutions are already developing structured information architectures to capture current and more accurate information about scientists&rsquo; interests, activities, and accomplishments<sup>3</sup>; and S&amp;T agencies in other countries have developed platforms to capture scientific outcomes.<sup>4</sup></p>
    <p>
    Spurred by the twin needs of responding to ARRA reporting requirements and the potential of using automated approaches to minimize reporting burden, OSTP staff and members of the Science of Science Policy (SOSP) Interagency Group worked with seven universities participating in the Federal Demonstration Partnership (FDP)  to develop a more reliable and low burden way of responding to ARRA.  The initial focus was to use university administrative records to identify all individuals supported by federal science funding in order to generate information on jobs created under ARRA.  The consensus by the seven universities was that it is not difficult to create the required files, and there is substantial value added from standardized reports. The results from the pilot were presented at the FDP national meeting in September 2009 and January 2010. The pilot demonstrated the feasibility of creating standardized measures of the results of science investments on initial job creation for pilot institutions with minimal burden on respondents.</p>
    <p>Both the agencies and research institutions involved expressed interest in expanding the pilot to create a data infrastructure that facilitated common responses to Congressional and Administrative requests.  That led to the formation of the current STAR METRICS Program, which continues to grow, and currently comprises 85 participating research institutions and six federal agencies.</p>
    <h2 style="text-align: center">
      <a name="level1"></a>2. Brief Overview of STAR METRICS Level I</h2>
    <p>
      Level I of the project documents the initial effect of S&amp;T investments on employment (<a href="https://starmetrics.nih.gov" target="_blank">starmetrics.nih.gov</a>).  It uses existing administrative data from participating research institutions to document four sets of job counts:</p>
    <ol>
      <li>The number of individuals (scientists, students and research staff on research organization payrolls) that are supported by Federal funds for scientific research. These numbers are also aggregated to generate full time equivalent (FTE) supported by federal S&amp;T dollars.</li>
      <li>The jobs that are generated by the expenditure of Federal funds on purchases from vendors.</li>
      <li>The jobs that are generated by the expenditure of Federal funds on sub-awards.</li>
      <li>The jobs that are generated by the expenditure of Federal funds for institutional support (through Facilities &amp; Administration costs).</li>
    </ol>
    <h2 style="text-align: center">
      <a name="level2"></a>3. STAR METRICS Level II</h2>
    <p>
      Level II is designed to capture inputs, outputs and outcomes beyond the initial employment effects described in Level I.  A series of consultative workshops helped to identify both the products that STAR METRICS should produce for science agencies together with the types of data, methods and tools that could be employed to generate the reports, while recognizing the continued importance of anecdotal and qualitative evidence. At least three major technical challenges were identified in the workshops: (1) the dispersed and unstructured location of relevant data elements, (2) the problem of attribution because of the wide and dispersed number of players, and (3) the long time lags intervening between funding and results. Workshop participants suggested a number of approaches that could be addressed by STAR METRICS to address these challenges.  Importantly, STAR METRICS could build on existing scientific knowledge by initially leveraging over $60 million of investments in academic research by the National Science Foundation&rsquo;s Science of Science and Innovation Policy (SciSIP) program, as well as the independent investments of many Federal agencies and research institutions. Equally importantly, STAR METRICS should begin by initiating some pilot projects, which could be built on and extended by the community as the program evolves.</p>
    <h2 style="text-align: center">
      <a name="pilotprojects"></a>4. STAR METRICS Pilot Projects</h2>
    <p>
    The STAR METRICS Portfolio Explorer tool (<a href="http://readidata.nitrd.gov/star/index.php">readidata.nitrd.gov/star/index.php</a>) provides a description of institutional scientific portfolios.  It makes use of natural language processing and topic modeling techniques to summarize the scientific topics described in each project description.  The tool can be used to answer <strong><em>funding agency</em></strong> questions such as: How much is being spent in each scientific area – so that growing and declining areas can be identified?  Where are the overlaps in funding across agencies – so that existing investments can be leveraged? The tool can be used to answer <strong><em>researcher</em></strong> questions such as: which funding agencies and programs are funding research like mine – so that the time looking for funding opportunities can be reduced? The tool can be used to answer <strong><em>university administrator</em></strong> questions such as: Where are my institutional research strengths and weaknesses – so that hiring decisions can be more strategic.  And it provides the  <strong><em>public and the media</em></strong> with<strong></strong> information about what research is being done in different scientific areas – so that they can access scientific knowledge in their own region.      </p>
    <p>
      The STAR METRICS Expertise Locator tool can be used to answer <strong><em>funding agency</em></strong> questions such as: Who are the experts – so that a broader base of proposal reviewers and workshop recipients can be developed? The tool can be used to answer <strong><em>researcher</em></strong> questions such as: who is doing research like me – so that new connections can be formed? The tool can be used to answer<strong><em> university administrator</em></strong> questions such as: Who is doing what research – so that scientific teams can be fostered?  It provides <strong><em>local businesses, the</em></strong> <strong><em>public and the media </em></strong>with<strong></strong>in formation about who is doing research in different scientific areas – so that links can be built between academic research and practical applications.      </p>
    <p>
      The STAR METRICS R&amp;D Dashboard (<a href="http://rd-dashboard.nitrd.gov/">rd-dashboard.nitrd.gov/</a>) provides place-based information in a manner symmetrical to the Portfolio Explorer and Expertise Locator, describing agency R&amp;D investment inputs by both topic and geographic area.  It begins to describe the outputs related to those investments by documenting the publications, patents and patent applications that can be associated with those investments.      </p>
    <p>
      However, publications, patents and patent applications offer a very limited view of the results of science investments. SciENcv is a Science Experts Network and CV platform is intended to provide an open and transparent way for the scientific community to document results more broadly by providing an automated approach to collecting information on researchers and their accomplishments. SciENcv, which represents a collaboration between two NSTC interagency groups – Research Business Methods and Science of Science Policy -- will serve as a vehicle for enabling discovery about researcher expertise, employment, education, and professional accomplishments.  It is also intended to reduce researcher and administrative burden for researchers and research institutions in federal grants administration. 
      </p>
    <p style="text-align: center"><img src="img/starMetrics.jpg" alt="Star Metrics" width="700" height="485" border="0"></p>

<p><sup>1</sup>  See Julia Lane and Stefano Bertuzzi &ldquo;Measuring the Results of Science Investments&rdquo; <strong><em>Science,</em></strong> Volume 331, 
&nbsp;&nbsp;&nbsp;pages 678-680, February 11, 2011, <a href="https://www.starmetrics.nih.gov/" target="_blank">starmetrics.nih.gov</a></p>
<p><sup>2</sup> <a href="http://www.whitehouse.gov/sites/default/files/omb/assets/memoranda_2010/m10-30.pdf" target="_blank">whitehouse.gov/sites/default/files/omb/assets/memoranda_2010/m10-30.pdf</a>;
&nbsp;&nbsp;<a href="http://www.whitehouse.gov/sites/default/files/omb/assets/memoranda_fy2009/m09-27.pdf" target="_blank">whitehouse.gov/sites/default/files/omb/assets/memoranda_fy2009/m09-27.pdf</a></p>
<p><sup>3</sup> Including, for example, the VIVO Project <a href="http://vivoweb.org" target="_blank">vivoweb.org</a> the Harvard Profiles System, and others.</p>
<p><sup>4</sup> For example, the Lattes platform has been adopted in 17 countries: see <a href="http://lattes.cnpq.br" target="_blank">lattes.cnpq.br</a>.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p style="text-align: right"><a href="about_topics.php">Topic Modeling &gt;</a><span style="text-align: right"></span></p>
<p>&nbsp; </p>
	<p>&nbsp; </p>
</div>